<div align="center">
  
  # Quantum Convolutional Neural Networks<br>for High-Energy Physics Analysis at the LHC
  
<a href="https://gist.github.com/eraraya-ricardo/8391f6bdae596e82fe0260c215c5ab8c" target="_blank"><img src="https://img.shields.io/badge/Google%20Summer%20of%20Code-2021-fbbc05?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAMAAABHPGVmAAAALVBMVEVHcEz7vQD7vQD8vQD7vQD8vQD7vQD8vQD8vQD7vQD7vQD8vQD7vQD7vQD7vQAgxtLpAAAADnRSTlMAZvVQ6QrVPhl6oSmHvzL6LQUAAASGSURBVHjatdnZdusgDAVQELMY%2Fv9zb2%2Bwc%2BIKDzQLvTXB3gYBFqmaDVeKU4sCBlFyy43WqLjlBpR1BpR1BpR1xjoFxmIFBpSVBpSVBpSVBpSVBpQ1xvdK1oPgblhfOWltjNaJq7ddYT2IfImYJqMDrENUChGDZn%2FWQ%2FMHxBcD4BMyBc5XCHkNQTq60vfIgXAx5xByju6T8V8itsT3%2FUPi6r39Ce8rp%2FCWYrHfIDXs95FZJs%2FvTob6Z4T2buQE4eikvHeG%2FoZY7TpRfDsNWzrjtP0L4s12NYhh%2BO1ZjJ9HfOjdYGo3QZx7YvwEAgOPdx3eQJlArMFA3wXSZ%2BwMQvplJGoPY6sqNU0gxcGYUVx5jtSIx3oS6HysTxEbMMDPAmkM9iFSXnPXt8nwuQ%2FYI8TH%2F425TQe7%2FnBPEH2bECI6T4t%2Bgvh4N1istR50FJdeIX1Ek%2FqJdGGQOWmAa4u7rn18vuuIzUq52gbxvpiSuzIau%2BuO9FUUfTvvCjcoQ4MMltRnEOqF0pdD%2FwiBZWxoqGCn8r2VGKIUCHOoTyHK2g7y1bsJRRqNe3%2FlXv5GbNhWEWXxbsf1UITRF4kYcM4KiI%2FbeFIevNNq7P2EIg0bVL%2BfqCcyYV2rbDdExWSPjUPPGBRh9JTowTscW0Dqf%2BwLXGmPthgKKMJo1f1OSQ29hf1Mbdlmg5NFV1H7KoICA3mruIQ4vl4TTFhvuAlxxrdb1J55KMJoBatEPCv6mr3sJzK%2F9RQKDAx49Ji5ctSLwsxAxgyuiduOAeVtIG14zppPKtAka9lcMZz71IHyNoAcCpvIx6UfxGLleCim3ggUpe0dQhe7I86mWvQERZmCIocryAqPsdYOSQlVIjCgyMRbLSaXxi3GD4LEw4AipzCyyvS5a5ThMpJTGAYUuQljhiWL53R11FN5BxhQsK0UWbE747E7evGV2FaEAUWmDave0H4LQxg6nErl1IEBBRdmOzjkBPpdqFB%2BpUtUGb0tDKloZP44hQLthQoDwXYiXlowpMJIymExdARL8SViYzymhGEMFR%2FR3cOyNoRCpQcZFu1s6AsNhlQuSiJP%2B1Kk90dNRHW9BYyhwlszhNgdb05CjmGcKDb3DotAoYIYV9wWxjDSZcHNmN%2Fj0KpPm3R7dMjq7HlrSokvjIqjww3SEhb4XJDpg3CLvM9%2BPG%2FMHOcaOwzYRFScNe8QHJb9nOEDhvkGwV48eZC3BgfzWwSHZaXthKEVMvkMaQnKhKESzSCkJ37uQqlJ7RmCIcbr%2By5qUEjiIwQK3q4yZKHqYDxEUIo4U6%2BNahxKr0kEZwv8HC%2BDqo69UaI2ieBAujN2RNhOoPybQjBr9oNSKNXSoQ%2B2luCUQuk1iSCIg9oiZl24Vv8TtXLROaotAtO3%2F9ooWSFcjDnH6BQio2SZQSRz%2FpsPfsifQ2RY1tmNBM3oxQRCbRjkOZn%2FEACT2J%2B1vkZiGESyG1SZS%2FqJ1wTogE1hEFHNh9yNCbvvREwqCwwoawwoKw0oKw0oKw0oKw0oKw0oKw0oMFYqMFYqMFYqMBYq88Y%2FxB7wiOJRvWkAAAAASUVORK5CYII%3D" /></a>
  
![version](https://img.shields.io/badge/version-1.0.0-blue)
[![License: MIT](https://img.shields.io/badge/License-MIT-red.svg)](https://opensource.org/licenses/MIT)
[![Open Source Love](https://firstcontributions.github.io/open-source-badges/badges/open-source-v2/open-source.svg)](https://github.com/firstcontributions/open-source-badges)

A Google Summer of Code 2021 Project Repository.<br>
This project aims to demonstrate quantum machine learning's potential, specifically Quantum Convolutional Neural Network (QCNN), in HEP events classification from particle image data.<br>The <b>code used in the research is wrapped as an open-source package</b> to ease future research in this field.<br><b>Check the [How to Use](#how-to-use) section to learn more about it.</b>

  <a href="https://ml4sci.org/" target="_blank"><img alt="gsoc@ml4sci" height="200px" src="https://raw.githubusercontent.com/eraraya-ricardo/GSoC-QCNN/main/assets/gsoc%40ml4sci.jpeg" /></a>
    
</div>

## Table of (Main) Contents
- [Introduction](#introduction)
  - [Abstract](#abstract)
- [How to Use](#how-to-use)
  - [Package Description](#package-description)
  - [Installation](#installation)
  - [Docs and Tutorial](#docs-and-tutorial)
- [Project's Datasets](#projects-datasets)
- [Research](#research)
  - [Results](#results)
- [References](#references)
- [Contributing and Reporting](#contributing-and-reporting)

## Introduction

- **Organization**
  - [Machine Learning for Science (ML4Sci)](https://ml4sci.org/)
- **Student**
  - [Eraraya Ricardo Muten](https://eraraya-ricardo.me/)
- **Mentors**
  - [Prof. Sergei V. Gleyzer](http://sergeigleyzer.com/), [Dr. Emanuele Usai](https://orcid.org/0000-0001-9323-2107), and [Raphael Koh](https://www.raphaelkoh.me/)
- **Project Details**
  - [Project Idea](https://ml4sci.org/gsoc/2021/proposal_QMLHEP2.html)
  - [Accepted Proposal](https://raw.githubusercontent.com/eraraya-ricardo/qml-hep-gsoc-2021/main/Eraraya_Ricardo_Muten_GSoC_2021_Proposal_QCNN.pdf)
  - [Official Project Page](https://summerofcode.withgoogle.com/projects/#5612096894533632)

### Abstract
One of the challenges in High-Energy Physics (HEP) is events classification, which is to predict whether an image of particle jets belongs to events being sought after or just background signals. Classical Convolutional Neural Network (CNN) has been proven a powerful algorithm in image classification, including jets image. As quantum computers promise many advantages over classical computing, comes a question on whether quantum machine learning (QML) can give any improvement in solving the problem. This project aims to demonstrate quantum machine learning's potential, specifically Quantum Convolutional Neural Network (QCNN), in HEP events classification from image data. The code used in the research is wrapped as an open-source package to ease future research in this field.
<!--Although many previous works have tried to classify images with QCNN, none of them is fully quantum. They were still incorporating classical fully-connected layers after variational circuits. This project will be one of the first to try classifying images with a fully quantum implementation of QCNN and probably the first one to do so with particle jets images.-->

## How to Use

### Package Description
This package is a [TensorFlow Quantum](https://www.tensorflow.org/quantum) implementation of quantum convolution and classifier with Data Re-uploading[[3](#references)] ansatz. Both are wrapped as [Keras](https://keras.io/) layers that can easily be integrated into other Keras layers (classical and/or quantum), acting as building blocks for Quantum Convolutional Neural Networks (both hybrid and fully quantum). The model can be trained using Keras API.

### Package Dependencies
<a href="https://www.python.org/" target="_blank"><img alt="Python" height="23px" src="https://img.shields.io/badge/Python%203.7-%2314354C.svg?style=for-the-badge&logo=python&logoColor=white" /></a>
<a href="https://quantumai.google/cirq" target="_blank"><img alt="Cirq" height="23px" src="https://img.shields.io/badge/Cirq%200.11.0-%23fff2c8.svg?style=for-the-badge&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMAAAADACAMAAABlApw1AAAACVBMVEUAAAD%2F%2F%2F8AAABzxoNxAAAAAnRSTlMAAHaTzTgAAAMbSURBVHja7d2BZjRJFIbhb8%2F9X%2FSywjL8f3V5TndNxykITOJ9ur4QMZL88%2FIzgN8IGMAABjCAAcRO%2FXxYnGycJwH%2Fx5UTAAD5QDgO%2BGxzAgEg3y%2FBAZjvBAdovxMcYPkucIDlOwEA0A%2BEA4Banw1nP0Dy%2FRIAAP1AeBhQOydAcADl%2ByU4wPuJ4ADK9x05gPL9Ehzg%2FURwAOQDAQCQn6RJUASA%2Fj4CASA%2FSTURegFXn37zXTlgfxa9O3LAfk1aCQ7I7h56dwQAeZJ%2BCQ6whiABALAeJjiAHr%2FvyAGU7wQH%2BHp8Rw7o%2F4GmNqAGaM%2FfIzigeT37BAXc8%2FizQ0ZAf%2F72JRCgcz1O6AdQ%2Fg7eAL4eJzDA16M7QoCvxwkG8PX4jhDg6%2FFLQICvxwkKgHwnIMDX4wQGQL4LHID5TggAeD1OSAiA%2BU4IATDfBQkAdD1OSAiA%2BU4IATDfBYkA8t%2BBfCUkBMDH74QYQPP9OOBovgPqaL8DPP%2BbATlx%2Bn8r8VbAT%2F73Axb53w9Y5L8AsOj%2FesAi%2F%2FsBi%2FwXABb97wNUvRtQAxjAAAYwgAEMYAADGMAABjCAAQxgAAMYwAAGMIABDGAAAxjAAAYwgAEM4PcC6t2AqqonAOff%2FO2As2%2B%2Fd0A7oT7OI4Dqzf84TwCq7uuvegDQQ6g%2FngcALqi%2FngcARqg6CXBCnQesCZR%2FH8AFdam%2FbgI44Vp%2BPQmoas5PPQ2o6s0%2FAKi%2B8dcZQFXX%2BI8BquLrOQuo0vXcDViH0HoEkCwB1wQF6zFAloDKNQKshwDJCnCVAPkGSD4BsCNYDwDyCbAdbb%2FIAckHwAj7r3BA8gmgHe1%2FEQdkCXDCQvfA39yNEdaf3Qhwwma%2BA6BtDV3KHbAQwI422A7oJuzkVxxwx452vA7ov4SNVwLACH5XlW5AZ9vFfAL4JcBJBHCeEAGcJyQGcEGJIHHAwUtIGHCUkF6AEyzfAS4I5APgBCF3AZwg%2BQ545BKSU4AeQk4CnJAcBti3QnIQ4JeQnAQ4IecBsqPkIMAvITkPkP86mHMAJyzyCfCeM4ABDGAAAxjAq8%2B%2F636IWYQLuCcAAAAASUVORK5CYII%3D&logoColor=black" /><a/>
<a href="https://www.tensorflow.org/" target="_blank"><img alt="TensorFlow" height="23px" src="https://img.shields.io/badge/TensorFlow%202.4.1-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white" /></a>
<a href="https://www.tensorflow.org/quantum" target="_blank"><img alt="TensorFlow Quantum" height="23px" src="https://img.shields.io/badge/TensorFlow--Quantum%200.5.1-%23425066.svg?style=for-the-badge&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAKgAAADNCAMAAAAbrMtvAAABMlBMVEUcR3oiSHcjSHcjSXckSXYlSXYmSHUnSXUoSXQrSXIrSnIuSnEwS3AxTG8yTG4yTG8zTG40TG40TW00TW41TW02TWw2TW03TWw5Tms8Tmk9Tmk%2FT2dAT2dAUGdBT2dBUGZBUGdCUGZDUGVDUGZDUWZEUGVFUWRFUWVGUWRKUmFKUmJMU2BTVF1WVFtYVVpbVlhbV1hiWlVsWk9tWk9vW05xW010XEt0XUt1XUt9YEaBYkSNYz2NZj2dZzWeZzShZzOhaDO7cyXCcCHDcSDDciDQcxrRdRnYdhXaeBTdehLiexDkew%2Flfg7oewzqewz6fgP6gAP%2BgwD%2FfgD%2FfwD%2FgAD%2FgQD%2FggD%2FgwD%2FhAD%2FhQD%2FhgD%2FhwD%2FiAD%2FiQD%2FigD%2F%2F%2F%2F5%2Bvr3%2Bfn09fb19vf29%2Fj%2F%2F%2F8Eoe3YAAAAZXRSTlMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADBQgI%2FPpD%2F74AAAR6SURBVHjaYoinFkhNjY%2BNi4sDUgBm56MAgSAIAJja%2BLfAjw63bUoUJERAVOsoj6i%2BUV6i9IzyEaVh1NcozaL8itIoyr%2BoLlEuonSIMhClPGowSmmU8ai6KFNRiqKmoxREWYmSHGU1KjPKRpS0qM0oKVH2o4RHORMVG%2BVYlLgoR6MERR2PEhAlIup0lKAoR6MCoxyLEht1Jkp4lP0oKVE2o9KibETJjLIYJTtqJUpBlNkoRVGmogqjDEepjRqLUh7lOkqLKBdRbaL8idIp6leUZlG%2BRWkY5SOqaZQbe%2FaO6zYMhGF0qSfN7H8Laa8Rxh758jXAfK2InwcqpaLQ6Dfa0IY2tKENbWhDi0H5PdR6KGIClLVQpkFZCDUVyiIos6FWQFkAZTrUIihToayDmgdlKZQ5UJZDmQC1BUoSmltcCfUbKBuhfAtlM5SvoA5AeQzlDNQzKMeg5KHMhD6XSkKthiakCShboHnqGMo2aJI6htoKTVDHUDZCM1JDKLuhCeq%2FUI5AE9RXqEPQBPUmaJSBRpSBRpSBRhlorIA2tKENbWhDG9rQhja0oQ1taEMbShkoZaDKQCkDpQxUGShloJSBUgaqDJQyUCKKQHkObWhDG9rQhka8dic0xl0GjXfdA41P3QGNTKeh%2BcpAI8pA4zooEtLj0LfD90A%2FTd8C%2FTx%2BBTQ1fwGUnPQw1LjboE%2FuOAh9eMkx6McvOldAx6j3Fx2AjkQJ6k5oQvPaQeiYknq%2BDJpgSh05D314aC%2F0T2boPDQ7cw804ou%2Fe8%2Bg%2FB4aiSZAKQNVBsoJaHwDpQyUMlBloJSBUgaqDJS3UBdB%2BS%2BUSEFtgjKGykHBJqgBFBhBhyR7oLxCgxTUjzZB%2BQElB%2FXaJqiI%2BNu8XWCpEQQBGI67r%2FvWrMZ94%2B6edKanBanI3v8KuEM1xbxC%2Fmc4H4xbFYoMaE8pBqG%2FBykupXVSyhhjrXXOee8zmUy2VC5XhiIyoCR1AqEElYLi%2BKD0n7o%2FeijS0BD17yRA%2Bec4jxFKU%2FtBUR4a%2BC4mlYBaNQYofVCJhr7LuniEUILK2Ju38vjJD2MEoUwpQaX3jy4cO7X97Ks3amhQJrV6l3LiAYC16YubL75lYyFouv3fzTvUPnwot3Tg2lNiBBCAMhfu5PUk2ITC7oWTjBGABSWlNLXrdTS0NgJ8d%2FJQ9iljAWcTWhsBLn%2B2Jh4OlKb%2BD0OxGwq7Z5YffUq0GJSW8qHYCwrR3Myp51mjxKC0IT201vLU3Tc%2BFoZiiPA%2F%2BCISGq0fPPc2Z5QAlLlw%2Fx96BQ0F2JpdffndKQEoT%2Fo%2FPNxpKETLB6588VqJQmnKv7CThpbaPXX9vTZKFIoDn6iJ%2FaEQzZ%2B49yuRhNIeKuRAAXaO7yWJEoViSmYYChuH7xujUkBFqMiHwvrR8tCPqwlBEdlMPhS2j%2B%2F91DWn1koGimwmHwob52%2Fculnp6oNfGSEoIpfJh8La7HS1S0cevrKGCWXEZtJQou2zp%2B%2B8LtE0E8qIgUwBhWj90OLtDx9NLme0DHSwqxiLfQiyQ6GK9nMAAAAASUVORK5CYII%3D&logoColor=white" /><a/>

### Installation
```shell
git clone https://github.com/eraraya-ricardo/qcnn-hep.git
cd qcnn-hep
python -m pip install -r requirements.txt
python setup.py
```

### Docs and Tutorial
- Main Tutorial: [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1TWQgdVzKUITMNzIUFtyzq9MfuTfkyJVe?usp=sharing) or click [here](https://github.com/eraraya-ricardo/qcnn-hep/blob/main/tutorials/Main%20Tutorial.ipynb).

## Weekly Progress
- Week 1: Looking and getting used to the dataset, train a classical ResNet[[1](#references)] model as a baseline.
- Week 2: Coding the graph-convolution preprocessing[[2](#references)] and Quantum Conv layer with data re-uploading[[3](#references)] PQC.
- Week 3: Testing the first iteration of the QCNN model, coding the parallelized convolution, testing ResNet with 8x8 images.
- Week 4: Tested the Kaggle platform, tested the parallelized convolution, trained QCNN with varying hyperparameters.
- Week 5: Tested the classical CNN and Fully-connected NN, started to train the QCNN v0.1.0 with varying filter size & stride, coded the new ansatz for quantum convolution layer based on [[4](#references)].
- Week 6: Tested the new quantum convolution ansatz[[4](#references)], try to combine ideas from data re-uploading circuit to the new ansatz, presented a short summary about the project at the MCQST Student Conference.
- Week 7: Tested the QCNN v0.1.1 on MNIST[[5](#references)] and LArTPC[[4](#references)] dataset.
- Week 8: Wrapped the code in the development notebooks as a Python package.
- Week 9: Tested the QCNN v0.1.1 and classical CNN on the Quark-Gluon[[6](#references)] dataset.
- Week 10: Cleaned up the repository, README, and the docs/tutorial notebook.
<!---
Notes:
- A **more detail progress and specific To-Do list** is made every week as an [issue](https://github.com/eraraya-ricardo/GSoC-QCNN/issues), covering all the comments and suggestions received during Wednesday & Friday meeting.
- Progress of the *N*-th week contains things that have been done in that week.
- To-Do list of the *N*-th week that is obtained from the meeting in that week is to be done on the next week (*N+1*-th week). When all the tasks in the To-Do list are done, the issue is marked as closed.
--->

## Project's Datasets

### Primary Dataset: Electromagnetic Calorimeter (ECAL) Dataset

<p align="middle">
  <img src="https://raw.githubusercontent.com/eraraya-ricardo/GSoC-QCNN/main/assets/photon%20full.png" title="Photon" />
  <img src="https://raw.githubusercontent.com/eraraya-ricardo/GSoC-QCNN/main/assets/electron%20full.png" title="Electron" /> <br>
  <a>Averages of Photon (left) and Electron (right) image samples from the dataset.</a>
</p>
  
The dataset contains images from two types of particles: photons (0) and electrons (1) captured by the ECAL detector.
- Each pixel corresponds to a detector cell.
- The intensity of the pixel corresponds to how much energy is measured in that cell.
- In total, there are 498,000 samples, equally distributed between the two classes.
- The size of the images are 32x32.

The dataset can be obtained [here](https://github.com/ML4SCIHackathon/ML4SCI/tree/main/ParticleImagesChallenge).

### Secondary Dataset
#### Liquid Argon Time Projection Chamber (LArTPC) Dataset[[4](#references)]

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/LArTPC_sample.png" title="LArTPC Dataset" /> <br>
  <a>An image sample for each class from LArTPC dataset.</a>
</p>

The dataset contains images of simulated particle activities (μ+, e−, p+, π+, π0, γ) in a LArTPC detector. This dataset is prepared by the authors for study in [[4](#references)].

- The images have a resolution of 480 x 600 pixels, where each pixel in the x-axis represents a single wire and each pixel in the y-axis represents a sampling time tick.
- Colors in the images represent the sizes of the ionization energy loss along the particle trajectories when measured by LArTPC’s wire planes.
- In total, there are 100 samples for each class.
- Each particle’s momentum is set such that the mean range of the particle is about 2 meters, so the classification is not sensitive to the image size.
- In this study, the images are scaled to 30x30, prepared by the original authors of the dataset.

The dataset can be obtained from the original authors of [[4](#references)] upon reasonable request.

#### Quark-Gluon Dataset [[6](#references)]

#### MNIST Dataset[[5](#references)]

<p align="middle">
  <img height="300 px" src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/mnist_sample.png" title="MNIST Dataset" /> <br>
  <a>An image sample for each class from MNIST dataset.</a>
</p>

The dataset contains images of grayscale (8 bit) handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples.

It can be obtained from [[5](#references)].

## Research

### Hardware and Platform
The whole project is run on `Google Colab` with `GPU` vary between `V100, P100, or T4`. The runtime listed in the [Research](#research) section might not be too accurate as the GPU used vary between runs. The main benchmarking metric is the `Test AUC`.

### Results
#### Early Testing
##### Test on ECAL Dataset

| Notebook Version Name  | Notes | Num. Trainable Params | Test AUC | Runtime (secs per epoch) |
| ------------- | ------------- | :-------------: | :-------------: | :-------------: |
| ResNet v2  | Whole samples with 15% for test samples, 200 epochs, 128 batch size, classical preprocessing: MinMax scaling -> subtract mean, optimizer: Adam(learning_rate=lr_schedule) | 295,074 | 0.80 | - |
| QCNN v0.1.0 (data re-uploading circuit) | Whole samples with 15% for test samples, 10 epochs, 128 batch size, 1 qubits, 1 layers, filter size = [3, 3], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling, optimizer: Adam(learning_rate=lr_schedule) | 190 | 0.730 | (about 1.5 hours/epoch) |
| ResNet v2  | Whole samples with 15% for test samples, 200 epochs, 128 batch size, classical preprocessing: crop to 8x8 -> MinMax scaling -> subtract mean, optimizer: Adam(learning_rate=lr_schedule) | 295,074 | 0.63 (overfit, train AUC = 0.80) | - |
| QCNN v0.2.0 (data re-uploading circuit) | Whole samples with 15% for test samples, 10 epochs, 128 batch size, 2 qubits, 2 layers, filter size = [2, 2], stride = [2, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling, optimizer: Adam(learning_rate=lr_schedule) | 194 | 0.68 | - |
| Classical CNN | Whole samples with 15% for test samples, 10 epochs, 128 batch size, filter size = [3, 3], stride = [1, 1], num. of filters = [2, 1], conv activation = [relu, relu], use_bias = [True, True], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling, optimizer: Adam(learning_rate=lr_schedule) | 193 | 0.738 | 13 |
| Classical Fully-connected NN | Whole samples with 15% for test samples, 10 epochs, 128 batch size, num. of nodes = [3, 2], activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling -> flatten to 64, optimizer: Adam(learning_rate=lr_schedule) | 203 | 0.691 | 10 |
<!-- | QCNN v1  | 10k samples with 15% for test samples, 200 epochs, 128 batch size, 1 qubits, *varying* layers, filter size = [3, 3], stride = [1, 1], followed by classical head [8, 2], classical preprocessing = Crop to 8x8, standard scaling | <ul><li>1 layer = 190</li><li>2 layers = 226</li><li>3 layers = 262</li><li>4 layers = </li>298</ul> | <ul><li>1 layer = ±0.636</li><li>2 layers = ±0.666</li><li>3 layers = ±0.622</li></ul> | <ul><li>1 layer = ±80 </li><li>2 layers = ±165 </li><li>3 layers = ±350 </li></ul> | -->

___

#### QCNN v0.1.1 (data re-uploading circuit[[3](#references)])
##### Test on ECAL Dataset
> 10k samples with 15% for test samples, 200 epochs, 128 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling

> optimizer: Adam(learning_rate=lr_schedule)

| Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Runtime (secs per epoch) |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 1 | 1 | 190 | 0.689 | 0.636 | ±80 |
| 1 | 2 | 226 | 0.716 | 0.666 | ±165 |
| 1 | 3 | 262 | 0.687 | 0.622 | ±330 |
| 1 | 4 | 298 | 0.691 | 0.607 | ±370 |
| 2 | 1 | 226 | 0.687 | 0.661 | ±200 |
| 2 | 2 | 298 | 0.710 | 0.645 | ±420 |
| 3 | 1 | 262 | 0.691 | 0.655 | ±350 |
| 3 | 2 | 370 | 0.707 | 0.636 | ±670 |

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/qcnn-v1.1_heatmap.png" title="Heatmap Representation of the Table Above" /> <br>
  <a>Validation AUC for Varying the Number of Layers and Qubits (0.0 = not tested).</a>
</p>

> 10k samples with 15% for test samples, 200 epochs, 128 batch size, *varying* qubits, *varying* layers, filter size = [2, 2], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling

| Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Runtime (secs per epoch) |
| :-------------: | ------------- | :-------------: | :-------------: | :-------------: | :-------------: |
| 1 | 1 | 338 | 0.650 | 0.623 | ±120 |

##### Test on LArTPC Dataset
> 2 classes, 160 training samples (80 per class), 40 testing samples (20 per class), 200 epochs, 16 batch size, *varying* qubits, *varying* layers, filter size = [3, 2], stride = [2, 2], followed by classical head [2] with activation [softmax], classical preprocessing: log scaling -> MinMax scaling

> optimizer: RMSProp(learning_rate=0.01, rho=0.99, epsilon=1e-08)

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/muon_electron.png" title="e- vs μ+" /> <br>
  <a>Sample Images of e- vs μ+.</a>
</p>

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/muon_proton.png" title="p+ vs μ+" /> <br>
  <a>Sample Images of p+ vs μ+.</a>
</p>

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/muon_pion-plus.png" title="π+ vs μ+" /> <br>
  <a>Sample Images of π+ vs μ+.</a>
</p>

| Classes | Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| :-------------: | ------------- | ------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| e- vs μ+ | 1 | 1 | 130 | 1.0 | 0.977 | 1.0 | 0.925 | ±6 |
| e- vs μ+ | 1 | 2 | 160 | 1.0 | 0.971 | 1.0 | 0.925 | ±14 |
| e- vs μ+ | 2 | 2 | 220 | 1.0 | 0.996 | 1.0 | 0.950 | ±27 |
| p+ vs μ+ | 1 | 1 | 130 | 1.0 | 0.980 | 1.0 | 0.950 | ±6 |
| p+ vs μ+ | 2 | 2 | 220 | 1.0 | 0.969 | 1.0 | 0.925 | ±25 |
| π+ vs μ+ | 1 | 1 | 130 | 1.0 | 0.928 | 1.0 | 0.850 | ±6 |
| π+ vs μ+ | 1 | 2 | 160 | 1.0 | 0.921 | 1.0 | 0.875 | ±11 |
| π+ vs μ+ | 1 | 3 | 190 | 1.0 | 0.863 | 1.0 | 0.825 | ±18 |
| π+ vs μ+ | 2 | 1 | 160 | 1.0 | 0.890 | 1.0 | 0.925 | ±12 |
| π+ vs μ+ | 2 | 2 | 220 | 1.0 | 0.977 | 1.0 | 0.950 | ±24 |
| π+ vs μ+ | 2 | 3 | 280 | 1.0 | 0.940 | 1.0 | 0.850 | ±36 |
| π+ vs μ+ | 3 | 1 | 190 | 1.0 | 0.896 | 1.0 | 0.875 | ±18 |
| π+ vs μ+ | 3 | 2 | 280 | 1.0 | 0.971 | 1.0 | 0.925 | ±35 |
| π+ vs μ+ | 3 | 3 | 370 | 1.0 | 0.954 | 1.0 | 0.850 | ±55 |
<!-- | p+ vs μ+ | 1 | 2 | 160 | 1.0 |  | 1.0 |  | ± | -->

\-----

> Results reported in [[4](#references)] (as a comparison). The model developed in this project produced similar results (with the QCNN) with less number of qubits and trainable parameters.

| Model | Classes | Num. Qubits | Num. Trainable Params | Train Accuracy | Test Accuracy |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| QCNN | e- vs μ+ | 9 | 472 | 1.0 | 0.925 |
| QCNN | p+ vs μ+ | 9 | 472 | 1.0 | 0.975 |
| QCNN | π+ vs μ+ | 9 | 472 | 0.9688 | 0.975 |
| CNN | e- vs μ+ | (classical) | 498 | 0.9938 | 0.95 |
| CNN | p+ vs μ+ | (classical) | 498 | 0.9125 | 0.80 |
| CNN | π+ vs μ+ | (classical) | 498 | 0.975 | 0.825 |

##### Test on Quark-Gluon Dataset
> **This part is still a working progress**. A much higher specs computational device (more RAMs) is needed for training the model because this dataset is huge.<br>Early testing with small samples showed a promising results. The training accuracies and AUCs are high, indicates that the model was able to learn how to differentiate the data. Low test metrics indicates overfitting, the model failed to generalize well -> we need to train the model on larger number of samples.
  
> 2 classes, 850 training samples (425 per class), 150 testing samples (75 per class), 200 epochs, 128 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [2, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: log scaling -> MinMax scaling

> optimizer: Adam(learning_rate=lr_schedule)

| Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| ------------ | ----------- | :-------------------: | :-------: | :------: | :------------: | :-----------: | :----------------------: |
| 1 | 1 | 2374 | 0.712 | 0.531 | 0.659 | 0.567 | 128 |
| 1 | 2 | 2410 | 0.928 | 0.559 | 0.846 | 0.567 | 370 |
| 1 | 3 | 2446 | 0.992 | 0.571 | 0.956 | 0.560 | 670 |
| 2 | 1 | 2410 | 0.829 | 0.663 | 0.754 | 0.640 | 450 |
  
##### Test on MNIST Dataset
> 2 classes, 400 training samples (200 per class), 1000 testing samples (500 per class), 10 epochs, 32 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [2, 2], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 27x27 -> pixels range [0, 1] (divide all pixels by 255)

> optimizer: Adam(learning_rate=lr_schedule)

| Classes | Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| :-------------: | ------------- | ------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 0 vs 1 | 1 | 1 | 350 | 0.999 | 0.999 | 0.99 | 0.99 | ±40 |

\-----

> 2 classes, 400 training samples (200 per class), 1000 testing samples (500 per class), 200 epochs, 32 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [2, 2], followed by classical head [2] with activation [softmax], classical preprocessing: pixels range [0, 1] (divide all pixels by 255)

> optimizer: Adam(learning_rate=0.001)

| Classes | Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| :-------------: | ------------- | ------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 0 vs 1 | 1 | 1 | 110 | 0.9999 | 0.9999 | 0.998 | 0.996 | ±41 |
| 3 vs 6 | 1 | 1 | 110 | 0.999 | 0.998 | 0.998 | 0.985 | ±55 |
| 4 vs 7 | 1 | 1 | 110 | 0.9999 | 0.993 | 0.998 | 0.967 | ±42 |
| 8 vs 9 | 1 | 1 | 110 | 0.997 | 0.978 | 0.975 | 0.927 | ±41 |
| 2 vs 5 | 1 | 1 | 110 | 0.9997 | 0.994 | 0.995 | 0.962 | ±41 |

___

#### QCNN v0.3.0 (circuit from [[4](#references)])
##### Test on ECAL Dataset
> 10k samples with 15% for test samples, 200 epochs, 128 batch size, *varying* layers, filter size = [3, 2], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> convert all pixels' value with arctan function

> optimizer: Adam(learning_rate=lr_schedule)

| Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Runtime (secs per epoch) |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 1 | 265 | 0.613 | 0.586 | ±300 |
| 2 | 304 | 0.663 | 0.644 | ±570 |
| 3 | 343 | 0.647 | 0.630 | ±780 |
| 4 | 382 | 0.653 | 0.635 | ±950 |

___

#### Classical CNN
##### Test on ECAL Dataset
> 10k samples with 15% for test samples, 200 epochs, 128 batch size, filter size = [3, 3], stride = [1, 1], conv activation = [relu, relu], use_bias = [True, True], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling

> optimizer: Adam(learning_rate=lr_schedule)

| Num. of filters | Num. Trainable Params | AUC Train | AUC Test | Runtime (secs per epoch) |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| [2, 1] | 193 | 0.723 | 0.675 | ±0.268 |
| [4, 1] | 231 | 0.735 | 0.693 | ±0.268 |
| [6, 1] | 269 | 0.745 | 0.696 | ±0.268 |
| [8, 1] | 307 | 0.746 | 0.700 | ±0.268 |
| [4, 2] | 396 | 0.764 | 0.699 | ±0.268 |
| [4, 3] | 561 | 0.784 | 0.687 | ±0.268 |

___

#### General Notes
1. **lr_schedule**

    ```
    lr = 1e-3
        if epoch > 180:
            lr *= 0.5e-3
        elif epoch > 160:
            lr *= 1e-3
        elif epoch > 120:
            lr *= 1e-2
        elif epoch > 80:
            lr *= 1e-1
    ```

## References
[1] [He, J. (2016). Identity Mappings in Deep Residual Networks. In Computer Vision – ECCV 2016 (pp. 630–645). Springer International Publishing.](https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38)

[2] [Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, & Shinjae Yoo. (2021). Hybrid Quantum-Classical Graph Convolutional Network.](https://arxiv.org/abs/2101.06189)

[3] [Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E., & Latorre, J. (2020). Data re-uploading for a universal quantum classifier. Quantum, 4, 226.](https://quantum-journal.org/papers/q-2020-02-06-226/)

[4] [Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, & Shinjae Yoo. (2020). Quantum Convolutional Neural Networks for High Energy Physics Data Analysis.](https://arxiv.org/abs/2012.12177)

[5] [LeCun Y, Cortes C. MNIST handwritten digit database 2010.](http://yann.lecun.com/exdb/mnist/)

[6] [Andrews, M., Alison, J., An, S., Burkle, B., Gleyzer, S., Narain, M., Paulini, M., Poczos, B., &amp; Usai, E. (2020). End-to-end jet classification of quarks and gluons with the cms open data. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 977, 164304.](https://www.sciencedirect.com/science/article/pii/S0168900220307002?via%3Dihub)

## Contributing and Reporting
### Contributing
If you want to contribute to the code (e.g., adding new features, provide another example of usage in Jupyter Notebook, research results), please let me know by sending a pull request with a comprehensive PR note on new things that you added and the reasoning. I won't be too strict on it as the main goal of this project is more towards "research" rather than "code development". So, as long as it is clear and good enough, I will merge the PR.

You can also open a [new issue](https://github.com/eraraya-ricardo/qcnn-hep/issues/new/choose) or contact me via [email](mailto:eraraya-ricardo@qlab.itb.ac.id) if you want to discuss things first.
  
### Reporting
If you used the code and found any bugs/errors, or have any suggestions, critics, requests, etc., please let me know either by opening up a [new issue](https://github.com/eraraya-ricardo/qcnn-hep/issues/new/choose) or by contacting me via [email](mailto:eraraya-ricardo@qlab.itb.ac.id).

Thank you!

<!---
Semantic Versioning 2.0.0: https://semver.org/
--->
