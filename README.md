<div align="center">
  
  # Quantum Convolutional Neural Networks<br>for High Energy Physics Analysis at the LHC
  
  <a href="https://summerofcode.withgoogle.com/projects/#5612096894533632" target="_blank"><img src="https://img.shields.io/badge/Google%20Summer%20of%20Code-2021-fbbc05?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAMAAABHPGVmAAAALVBMVEVHcEz7vQD7vQD8vQD7vQD8vQD7vQD8vQD8vQD7vQD7vQD8vQD7vQD7vQD7vQAgxtLpAAAADnRSTlMAZvVQ6QrVPhl6oSmHvzL6LQUAAASGSURBVHjatdnZdusgDAVQELMY%2Fv9zb2%2Bwc%2BIKDzQLvTXB3gYBFqmaDVeKU4sCBlFyy43WqLjlBpR1BpR1BpR1xjoFxmIFBpSVBpSVBpSVBpSVBpQ1xvdK1oPgblhfOWltjNaJq7ddYT2IfImYJqMDrENUChGDZn%2FWQ%2FMHxBcD4BMyBc5XCHkNQTq60vfIgXAx5xByju6T8V8itsT3%2FUPi6r39Ce8rp%2FCWYrHfIDXs95FZJs%2FvTob6Z4T2buQE4eikvHeG%2FoZY7TpRfDsNWzrjtP0L4s12NYhh%2BO1ZjJ9HfOjdYGo3QZx7YvwEAgOPdx3eQJlArMFA3wXSZ%2BwMQvplJGoPY6sqNU0gxcGYUVx5jtSIx3oS6HysTxEbMMDPAmkM9iFSXnPXt8nwuQ%2FYI8TH%2F425TQe7%2FnBPEH2bECI6T4t%2Bgvh4N1istR50FJdeIX1Ek%2FqJdGGQOWmAa4u7rn18vuuIzUq52gbxvpiSuzIau%2BuO9FUUfTvvCjcoQ4MMltRnEOqF0pdD%2FwiBZWxoqGCn8r2VGKIUCHOoTyHK2g7y1bsJRRqNe3%2FlXv5GbNhWEWXxbsf1UITRF4kYcM4KiI%2FbeFIevNNq7P2EIg0bVL%2BfqCcyYV2rbDdExWSPjUPPGBRh9JTowTscW0Dqf%2BwLXGmPthgKKMJo1f1OSQ29hf1Mbdlmg5NFV1H7KoICA3mruIQ4vl4TTFhvuAlxxrdb1J55KMJoBatEPCv6mr3sJzK%2F9RQKDAx49Ji5ctSLwsxAxgyuiduOAeVtIG14zppPKtAka9lcMZz71IHyNoAcCpvIx6UfxGLleCim3ggUpe0dQhe7I86mWvQERZmCIocryAqPsdYOSQlVIjCgyMRbLSaXxi3GD4LEw4AipzCyyvS5a5ThMpJTGAYUuQljhiWL53R11FN5BxhQsK0UWbE747E7evGV2FaEAUWmDave0H4LQxg6nErl1IEBBRdmOzjkBPpdqFB%2BpUtUGb0tDKloZP44hQLthQoDwXYiXlowpMJIymExdARL8SViYzymhGEMFR%2FR3cOyNoRCpQcZFu1s6AsNhlQuSiJP%2B1Kk90dNRHW9BYyhwlszhNgdb05CjmGcKDb3DotAoYIYV9wWxjDSZcHNmN%2Fj0KpPm3R7dMjq7HlrSokvjIqjww3SEhb4XJDpg3CLvM9%2BPG%2FMHOcaOwzYRFScNe8QHJb9nOEDhvkGwV48eZC3BgfzWwSHZaXthKEVMvkMaQnKhKESzSCkJ37uQqlJ7RmCIcbr%2By5qUEjiIwQK3q4yZKHqYDxEUIo4U6%2BNahxKr0kEZwv8HC%2BDqo69UaI2ieBAujN2RNhOoPybQjBr9oNSKNXSoQ%2B2luCUQuk1iSCIg9oiZl24Vv8TtXLROaotAtO3%2F9ooWSFcjDnH6BQio2SZQSRz%2FpsPfsifQ2RY1tmNBM3oxQRCbRjkOZn%2FEACT2J%2B1vkZiGESyG1SZS%2FqJ1wTogE1hEFHNh9yNCbvvREwqCwwoawwoKw0oKw0oKw0oKw0oKw0oKw0oMFYqMFYqMFYqMBYq88Y%2FxB7wiOJRvWkAAAAASUVORK5CYII%3D" /></a>
  
  <a href="https://ml4sci.org/" target="_blank"><img alt="gsoc@ml4sci" height="200px" src="https://raw.githubusercontent.com/eraraya-ricardo/GSoC-QCNN/main/assets/gsoc%40ml4sci.jpeg" /></a>
    
</div>

## Organization

[Machine Learning for Science (ML4Sci)](https://ml4sci.org/)

## Mentors

[Prof. Sergei V. Gleyzer](http://sergeigleyzer.com/), [Dr. Emanuele Usai](https://orcid.org/0000-0001-9323-2107), and [Raphael Koh](https://www.raphaelkoh.me/)

## Project Proposal
- [Project idea from the organization](https://ml4sci.org/gsoc/2021/proposal_QMLHEP2.html).
- [Proposal from the student](https://github.com/eraraya-ricardo/qml-hep-gsoc-2021/blob/main/Eraraya_Ricardo_Muten_GSoC_2021_Proposal_QCNN.pdf).

## Abstract

One of the challenges in High-Energy Physics (HEP) is events classification, which is to predict whether an image of particle jets belongs to events being sought after or just background signals. Classical Convolutional Neural Network (CNN) has been proven a powerful algorithm in image classification, including jets image. As quantum computers promise many advantages over classical computing, comes a question on whether quantum machine learning (QML) can give any improvement in solving the problem.

This project aims to demonstrate quantum machine learning's potential, specifically Quantum Convolutional Neural Network (QCNN), in HEP events classification from image data. Although many previous works have tried to classify images with QCNN, none of them is fully quantum. They were still incorporating classical fully-connected layers after variational circuits. This project will be one of the first to try classifying images with a fully quantum implementation of QCNN and probably the first one to do so with particle jets images.

## Dataset

### Primary Dataset: Electromagnetic Calorimeter (ECAL) Dataset

<p align="middle">
  <img src="https://raw.githubusercontent.com/eraraya-ricardo/GSoC-QCNN/main/assets/photon%20full.png" title="Photon" />
  <img src="https://raw.githubusercontent.com/eraraya-ricardo/GSoC-QCNN/main/assets/electron%20full.png" title="Electron" /> <br>
  <a>Averages of Photon (left) and Electron (right) image samples from the dataset.</a>
</p>
  
The dataset contains images from two types of particles: photons (0) and electrons (1) captured by the ECAL detector.
- Each pixel corresponds to a detector cell.
- The intensity of the pixel corresponds to how much energy is measured in that cell.
- In total, there are 498,000 samples, equally distributed between the two classes.
- The size of the images are 32x32.

The dataset can be obtained [here](https://github.com/ML4SCIHackathon/ML4SCI/tree/main/ParticleImagesChallenge).

### Secondary Dataset
#### Liquid Argon Time Projection Chamber (LArTPC) Dataset[[4](#references)]

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/LArTPC_sample.png" title="LArTPC Dataset" /> <br>
  <a>An image sample for each class from LArTPC dataset.</a>
</p>

The dataset contains images of simulated particle activities (μ+, e−, p+, π+, π0, γ) in a LArTPC detector. This dataset is prepared by the authors for study in [[4](#references)].

- The images have a resolution of 480 x 600 pixels, where each pixel in the x-axis represents a single wire and each pixel in the y-axis represents a sampling time tick.
- Colors in the images represent the sizes of the ionization energy loss along the particle trajectories when measured by LArTPC’s wire planes.
- In total, there are 100 samples for each class.
- Each particle’s momentum is set such that the mean range of the particle is about 2 meters, so the classification is not sensitive to the image size.
- In this study, the images are scaled to 30x30, prepared by the original authors of the dataset.

The dataset can be obtained from the original authors of [[4](#references)] upon reasonable request.

#### MNIST Dataset[[5](#references)]

<p align="middle">
  <img height="300 px" src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/mnist_sample.png" title="MNIST Dataset" /> <br>
  <a>An image sample for each class from MNIST dataset.</a>
</p>

The dataset contains images of grayscale (8 bit) handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples.

It can be obtained from [[5](#references)].

## Weekly Progress
- Week 1: Looking and getting used to the dataset, train a classical ResNet[[1](#references)] model as a baseline.
- Week 2: Coding the graph-convolution preprocessing[[2](#references)] and Quantum Conv layer with data re-uploading[[3](#references)] PQC.
- Week 3: Testing the first iteration of the QCNN model, coding the parallelized convolution, testing ResNet with 8x8 images.
- Week 4: Tested the Kaggle platform, tested the parallelized convolution, trained QCNN with varying hyperparameters.
- Week 5: Tested the classical CNN and Fully-connected NN, started to train the QCNN v.1 with varying filter size & stride, coded the new ansatz for quantum convolution layer based on [[4](#references)].
- Week 6: Tested the new quantum convolution ansatz[[4](#references)], try to combine ideas from data re-uploading circuit to the new ansatz, presented a short summary about the project at the MCQST Student Conference.
- Week 7: Tested the QCNN v1.1 on MNIST[[5](#references)] and LArTPC[[4](#references)] dataset.
- Week 8: (on progress).

Notes:
- A **more detail progress and specific To-Do list** is made every week as an [issue](https://github.com/eraraya-ricardo/GSoC-QCNN/issues), covering all the comments and suggestions received during Wednesday & Friday meeting.
- Progress of the *N*-th week contains things that have been done in that week.
- To-Do list of the *N*-th week that is obtained from the meeting in that week is to be done on the next week (*N+1*-th week). When all the tasks in the To-Do list are done, the issue is marked as closed.

## Research
### Results
#### Early Testing
##### Test on ECAL Dataset

| Notebook Version Name  | Notes | Num. Trainable Params | Test AUC | Runtime (secs per epoch) |
| ------------- | ------------- | :-------------: | :-------------: | :-------------: |
| ResNet v2  | Whole samples with 15% for test samples, 200 epochs, 128 batch size, classical preprocessing: MinMax scaling -> subtract mean, optimizer: Adam(learning_rate=lr_schedule) | 295,074 | 0.80 | - |
| QCNN v1 (data re-uploading circuit) | Whole samples with 15% for test samples, 10 epochs, 128 batch size, 1 qubits, 1 layers, filter size = [3, 3], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling, optimizer: Adam(learning_rate=lr_schedule) | 190 | 0.730 | (about 1.5 hours/epoch) |
| ResNet v2  | Whole samples with 15% for test samples, 200 epochs, 128 batch size, classical preprocessing: crop to 8x8 -> MinMax scaling -> subtract mean, optimizer: Adam(learning_rate=lr_schedule) | 295,074 | 0.63 (overfit, train AUC = 0.80) | - |
| QCNN v2 (data re-uploading circuit) | Whole samples with 15% for test samples, 10 epochs, 128 batch size, 2 qubits, 2 layers, filter size = [2, 2], stride = [2, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling, optimizer: Adam(learning_rate=lr_schedule) | 194 | 0.68 | - |
| Classical CNN | Whole samples with 15% for test samples, 10 epochs, 128 batch size, filter size = [3, 3], stride = [1, 1], num. of filters = [2, 1], conv activation = [relu, relu], use_bias = [True, True], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling, optimizer: Adam(learning_rate=lr_schedule) | 193 | 0.738 | 13 |
| Classical Fully-connected NN | Whole samples with 15% for test samples, 10 epochs, 128 batch size, num. of nodes = [3, 2], activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling -> flatten to 64, optimizer: Adam(learning_rate=lr_schedule) | 203 | 0.691 | 10 |
<!-- | QCNN v1  | 10k samples with 15% for test samples, 200 epochs, 128 batch size, 1 qubits, *varying* layers, filter size = [3, 3], stride = [1, 1], followed by classical head [8, 2], classical preprocessing = Crop to 8x8, standard scaling | <ul><li>1 layer = 190</li><li>2 layers = 226</li><li>3 layers = 262</li><li>4 layers = </li>298</ul> | <ul><li>1 layer = ±0.636</li><li>2 layers = ±0.666</li><li>3 layers = ±0.622</li></ul> | <ul><li>1 layer = ±80 </li><li>2 layers = ±165 </li><li>3 layers = ±350 </li></ul> | -->

___

#### QCNN v1.1 (data re-uploading circuit[[3](#references)])
##### Test on ECAL Dataset
> 10k samples with 15% for test samples, 200 epochs, 128 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling

> optimizer: Adam(learning_rate=lr_schedule)

| Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Runtime (secs per epoch) |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 1 | 1 | 190 | 0.689 | 0.636 | ±80 |
| 1 | 2 | 226 | 0.716 | 0.666 | ±165 |
| 1 | 3 | 262 | 0.687 | 0.622 | ±330 |
| 1 | 4 | 298 | 0.691 | 0.607 | ±370 |
| 2 | 1 | 226 | 0.687 | 0.661 | ±200 |
| 2 | 2 | 298 | 0.710 | 0.645 | ±420 |
| 3 | 1 | 262 | 0.691 | 0.655 | ±350 |
| 3 | 2 | 370 | 0.707 | 0.636 | ±670 |

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/qcnn-v1.1_heatmap.png" title="Heatmap Representation of the Table Above" /> <br>
  <a>Validation AUC for Varying the Number of Layers and Qubits (0.0 = not tested).</a>
</p>

> 10k samples with 15% for test samples, 200 epochs, 128 batch size, *varying* qubits, *varying* layers, filter size = [2, 2], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling

| Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Runtime (secs per epoch) |
| :-------------: | ------------- | :-------------: | :-------------: | :-------------: | :-------------: |
| 1 | 1 | 338 | 0.650 | 0.623 | ±120 |

##### Test on LArTPC Dataset
> 2 classes, 160 training samples (80 per class), 40 testing samples (20 per class), 200 epochs, 16 batch size, *varying* qubits, *varying* layers, filter size = [3, 2], stride = [2, 2], followed by classical head [2] with activation [softmax], classical preprocessing: log scaling -> MinMax scaling

> optimizer: RMSProp(learning_rate=0.01, rho=0.99, epsilon=1e-08)

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/muon_electron.png" title="e- vs μ+" /> <br>
  <a>Sample Images of e- vs μ+.</a>
</p>

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/muon_proton.png" title="p+ vs μ+" /> <br>
  <a>Sample Images of p+ vs μ+.</a>
</p>

<p align="middle">
  <img src="https://github.com/eraraya-ricardo/GSoC-QCNN/blob/main/assets/muon_pion-plus.png" title="π+ vs μ+" /> <br>
  <a>Sample Images of π+ vs μ+.</a>
</p>

| Classes | Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| :-------------: | ------------- | ------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| e- vs μ+ | 1 | 1 | 130 | 1.0 | 0.977 | 1.0 | 0.925 | ±6 |
| p+ vs μ+ | 1 | 1 | 130 | 1.0 | 0.980 | 1.0 | 0.950 | ±6 |
| π+ vs μ+ | 1 | 1 | 130 | 1.0 | 0.928 | 1.0 | 0.850 | ±6 |
| π+ vs μ+ | 1 | 2 | 160 | 1.0 | 0.921 | 1.0 | 0.875 | ±11 |
| π+ vs μ+ | 1 | 3 | 190 | 1.0 | 0.863 | 1.0 | 0.825 | ±18 |
| π+ vs μ+ | 2 | 1 | 160 | 1.0 | 0.890 | 1.0 | 0.925 | ±12 |
| π+ vs μ+ | 2 | 2 | 220 | 1.0 | 0.977 | 1.0 | 0.950 | ±24 |
| π+ vs μ+ | 2 | 3 | 280 | 1.0 | 0.940 | 1.0 | 0.850 | ±36 |
| π+ vs μ+ | 3 | 1 | 190 |  |  |  |  | ± |

\-----

> Results reported in [[4](#references)] (as a comparison). The model developed in this project produced similar results (with the QCNN) with less number of qubits and trainable parameters.

| Model | Classes | Num. Qubits | Num. Trainable Params | Train Accuracy | Test Accuracy |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| QCNN | e- vs μ+ | 9 | 472 | 1.0 | 0.925 |
| QCNN | p+ vs μ+ | 9 | 472 | 1.0 | 0.975 |
| QCNN | π+ vs μ+ | 9 | 472 | 0.9688 | 0.975 |
| CNN | e- vs μ+ | (classical) | 498 | 0.9938 | 0.95 |
| CNN | p+ vs μ+ | (classical) | 498 | 0.9125 | 0.80 |
| CNN | π+ vs μ+ | (classical) | 498 | 0.975 | 0.825 |

##### Test on MNIST Dataset
> 2 classes, 400 training samples (200 per class), 1000 testing samples (500 per class), 10 epochs, 32 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [2, 2], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 27x27 -> pixels range [0, 1] (divide all pixels by 255)

> optimizer: Adam(learning_rate=lr_schedule)

| Classes | Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| :-------------: | ------------- | ------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 0 vs 1 | 1 | 1 | 350 | 0.999 | 0.999 | 0.99 | 0.99 | ±40 |

\-----

> 2 classes, 400 training samples (200 per class), 1000 testing samples (500 per class), 200 epochs, 32 batch size, *varying* qubits, *varying* layers, filter size = [3, 3], stride = [2, 2], followed by classical head [2] with activation [softmax], classical preprocessing: pixels range [0, 1] (divide all pixels by 255)

> optimizer: Adam(learning_rate=0.001)

| Classes | Num. Qubits  | Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Train Accuracy | Test Accuracy | Runtime (secs per epoch) |
| :-------------: | ------------- | ------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 0 vs 1 | 1 | 1 | 110 | 0.998 | 0.995 | 0.991 | 0.974 | ±57 |
| 3 vs 6 | 1 | 1 | 110 | 0.999 | 0.998 | 0.998 | 0.985 | ±55 |
| 4 vs 7 | 1 | 1 | 110 | 0.9999 | 0.993 | 0.998 | 0.967 | ±42 |
| 8 vs 9 | 1 | 1 | 110 | 0.997 | 0.978 | 0.975 | 0.927 | ±41 |
| 2 vs 5 | 1 | 1 | 110 |  |  |  |  | ± |

___

#### QCNN v3 (circuit from [[4](#references)])
##### Test on ECAL Dataset
> 10k samples with 15% for test samples, 200 epochs, 128 batch size, *varying* layers, filter size = [3, 2], stride = [1, 1], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> convert all pixels' value with arctan function

> optimizer: Adam(learning_rate=lr_schedule)

| Num. Layers | Num. Trainable Params | Train AUC | Test AUC | Runtime (secs per epoch) |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| 1 | 265 | 0.613 | 0.586 | ±300 |
| 2 | 304 | 0.663 | 0.644 | ±570 |
| 3 | 343 | 0.647 | 0.630 | ±780 |
| 4 | 382 | 0.653 | 0.635 | ±950 |

___

#### Classical CNN
##### Test on ECAL Dataset
> 10k samples with 15% for test samples, 200 epochs, 128 batch size, filter size = [3, 3], stride = [1, 1], conv activation = [relu, relu], use_bias = [True, True], followed by classical head [8, 2] with activation [relu, softmax], classical preprocessing: crop to 8x8 -> standard scaling

> optimizer: Adam(learning_rate=lr_schedule)

| Num. of filters | Num. Trainable Params | AUC Train | AUC Test | Runtime (secs per epoch) |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
| [2, 1] | 193 | 0.723 | 0.675 | ±0.268 |
| [4, 1] | 231 | 0.735 | 0.693 | ±0.268 |
| [6, 1] | 269 | 0.745 | 0.696 | ±0.268 |
| [8, 1] | 307 | 0.746 | 0.700 | ±0.268 |
| [4, 2] | 396 | 0.764 | 0.699 | ±0.268 |
| [4, 3] | 561 | 0.784 | 0.687 | ±0.268 |

## Package Dependencies
- Python 3.7.10
- TensorFlow 2.4.1
- TensorFlow Quantum 0.5.1
- Cirq 0.11.0
- Sympy 1.5
- Numpy 1.19.5

## Hardware and Platform
The whole project is run on `Google Colab` with `GPU` vary between `V100, P100, or T4`. The runtime listed in the [Research](#research) section might not be too accurate as the GPU used vary between runs. The main benchmarking metric is the `Test AUC`.

## References
[1] [He, J. (2016). Identity Mappings in Deep Residual Networks. In Computer Vision – ECCV 2016 (pp. 630–645). Springer International Publishing.](https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38)

[2] [Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, & Shinjae Yoo. (2021). Hybrid Quantum-Classical Graph Convolutional Network.](https://arxiv.org/abs/2101.06189)

[3] [Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E., & Latorre, J. (2020). Data re-uploading for a universal quantum classifier. Quantum, 4, 226.](https://quantum-journal.org/papers/q-2020-02-06-226/)

[4] [Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, & Shinjae Yoo. (2020). Quantum Convolutional Neural Networks for High Energy Physics Data Analysis.](https://arxiv.org/abs/2012.12177)

[5] [LeCun Y, Cortes C. MNIST handwritten digit database 2010.](http://yann.lecun.com/exdb/mnist/)
